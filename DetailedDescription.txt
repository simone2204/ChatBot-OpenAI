Let's start describing all the libraries used and their main purpose:

JSON: mainly used to save the file name of the PDF documents, in order to make the application remember the last file shared by the user to not reloading it everytime.
OS: used to retrieve API key saved in the system. Copy and paste API key in the code is not reliable.
SYS: used in class "main()" (rows 267 and 270) to initialize the Application PyQt5 and close it with "exit" command.
FITZ: used to read pdf files.
PyQt5: used to create Windows application with a good UI interface, and also for threads and signals.
LANGCHAIN_OPENAI: mainly used for OpenAI Embeddings and ChatOpenAI, in order to use their embeddings and AI models.
LANGCHAIN_COMMUNITY.VECTORSTORES: used for the FAISS (Facebook AI Similarity Search) to create the index of the text. Essential.
LANGCHAIN.TEXT_SPLITTER: used to split the text in chunks.

Now let's describe the flow of the program:

When the application is launched, we are in the "MainWindow()" class (row 105), which inherits from the class "QMainWindow" in order to acquire all the behaviour of a Main Window, such as
toolbar and widgets. Here I initialized two important variables: pdf_text and qa_chain. The first is going to store the text from the document, the latter will be the main chain used
to retrieve informations. We will see better his utility further on. Then i define the model I want to use from OpenAI with "chat_model" variable. I chose GPT-3.5-Turbo model.

Program Flow Explanation
The first method called is initUI(), which initializes the main graphical interface of the application
using PyQt5 widgets.
Since PyQt5 is not the main focus here, I won't go into much detail about the UI components.
When the application starts, initUI() calls the method load_existing_index().
This function checks whether a previously created FAISS index exists.
If it does, the application loads it into the variable qa_chain, making it available for future queries.
It also loads the previously saved filename and displays it in the UI toolbar.
This method runs only once at startup.
Loading and Processing a PDF
The method load_pdf() is triggered when the user selects a PDF file.
- It opens a file dialog to allow the user to choose a PDF.
- It saves the filename in a JSON file using the save_filename() method.
- It then starts a background thread PdfLoaderThread() to extract the text content from the PDF and
show progress via a progress bar.
- Once the text extraction is complete, the finished signal triggers the on_pdf_loaded() method.
In on_pdf_loaded():
- The extracted text is stored in self.pdf_text.
- The saved filename is reloaded from the JSON file and shown in the UI.
- A new background thread IndexingThread() is started to index the extracted text.
Indexing the Text
The IndexingThread is the most important thread in the application.
It handles the vectorization of the document using LangChain and OpenAI embeddings.
Here?s what happens:
1. A RecursiveCharacterTextSplitter is initialized to divide the text into chunks:
- chunk_size=2000: each chunk contains up to 2000 characters.
- chunk_overlap=200: overlapping text between chunks helps preserve context.
2. The text is split into smaller documents using splitter.create_documents([self.pdf_text]).
3. An instance of OpenAIEmbeddings() is created to convert these documents into embeddings,
which are numerical representations of language.
4. These embeddings are stored using FAISS: vectorstore = FAISS.from_documents(documents,
embeddings)
5. The FAISS index is saved locally with vectorstore.save_local().
When indexing is finished, on_indexing_finished() is called.
- This method connects the FAISS vectorstore to the variable qa_chain using:
self.qa_chain = vectorstore.as_retriever(search_kwargs={"k": 20})
- qa_chain now becomes a retriever, capable of searching through the indexed content for the most
relevant chunks related to user queries.
Asking a Question
Once the document is indexed, the user can type a question and press the "Send" button.
- This triggers the ask_chatbot() method.
- The user's input is collected from the input_box and stored in user_input.
- The call to QApplication.processEvents() ensures the UI remains responsive while the model
processes the request.
- A new thread QueryThread() is started, receiving:
- qa_chain ? the retriever
- user_input ? the question
- chat_model ? the AI model (GPT-3.5-turbo)
Generating a Response
Inside QueryThread, the following steps occur:
1. The retriever calls .invoke(user_input) to get the most relevant chunks from the document.
- These are stored in relevant_docs.
- It's useful to print the length of this context for debugging or analysis.
2. A list named messages is created:
- It follows the OpenAI chat completion format.
- Each element is a dictionary containing:
- "role" (e.g., "system" or "user")
- "content" (text instructions or the user?s question)
- These messages define the prompt structure to guide the model's response.
3. The model is invoked using: response = self.chat_model.invoke(messages)
4. The result is stored in response, which contains the answer generated by the AI.
5. The answer is finally shown in the UI output box.
Final Thoughts
The program follows a clean and modular structure:
- Threads handle long operations like loading, indexing, and querying, keeping the UI responsive.
- The combination of FAISS + LangChain + OpenAI enables powerful retrieval-augmented question
answering (RAG).
- The chatbot provides document-based answers without requiring the model to "see" the entire PDF
at o



